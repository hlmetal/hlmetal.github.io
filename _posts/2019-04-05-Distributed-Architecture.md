---
layout: post
title:  "分布式架构之浅谈"
date:   2019-04-01 09:00:30 +0200
categories: 分布式架构
---

分布式架构优缺点、技术栈以及分布式架构中的各种机制, 如容错、性能等设计思路

## 分布式架构
### 优点
* 增大系统容量，随着业务量增大，必须要多台机器才能应对，要将业务系统水平或垂直拆分； 大流量处理： 通过集群技术把大规模并发请求的负载分散到不同的机器上
* 加强系统可用性，随着业务越来越关键，就不能出现单点故障，要通过分布式架构冗余系统以消除单点故障；关键业务保护，把故障隔离起来阻止雪崩效应，如果流量过大，需要对业务降级，以保护关键业务流转
* 模块化，系统重用性高、扩展性高、耦合度低
* 服务拆分，开发和发布速度变快
* 团队协作得到改善

### 缺点
* 架构设计复杂、服务部署复杂
* 技术多元化，运维、测试、查错复杂度增加
* 服务治理和调度变得困难
* 系统响应时间变长
* 线上故障工单解决变慢，会在不同服务和团队中转来转去\

### 难点
* 异构系统的不标准问题， 例如：通信协议、数据格式不标准
* 系统架构中的服务依赖性问题，木桶短板效应
* 故障发生的概率更大
* 多层架构的运维复杂度更大，基础层、平台层、应用层和接入层，任何一层的问题都会导致整体的问题

### 技术栈
* 提高架构稳定性
    * 服务拆分
    * 服务冗余
    * 限流降级
    * 高可用架构、高可用运维
* 提高架构性能
    * 缓存系统
    * 负载均衡系统
    * 异步调用
    * 数据分区、数据镜像

### 关键技术
* 全栈系统监控
    * 基础层：监控主机和底层资源， 例如： CPU、内存、网络吞吐、硬盘 I/O
    * 中间层：中间件层的监控， 例如： Nginx、Redis、ActiveMQ、Kafka、MySQL、Tomcat
    * 应用层：监控应用层的使用， 例如： HTTP 访问的吞吐量、响应时间、返回码、调用链路分析
    * 标准化：日志数据结构化、监控数据格式标准化、统一的监控平台、统一的日志分析
    * 好的监控系统: 关注于整体应用的 SLA、关联指标聚合、快速故障定位
    * 怎么做: 服务调用链跟踪、服务调用时长分布、服务的 TOP N 视图、数据库操作关联、服务资源跟踪
* 服务 / 资源调度
    * 服务关键程度、服务依赖关系图
    * 服务状态和生命周期的管理：服务注册中心
    * 架构的版本管理
    * 资源 / 服务调度、服务状态的维持和拟合、服务的弹性伸缩和故障迁移
* 流量调度
    * 服务流控，服务发现、服务路由、服务降级、服务熔断、服务保护
    * 流量控制，负载均衡、流量分配、流量控制、灾备多活
    * 流量管理，协议转换、请求校验、数据缓存、数据计算
* 状态 / 数据调度
    * 应用层上的分布式事务一致性，只有两阶段提交这样的方式
    * 底层存储可以解决这个问题的方式是通过一些像 Paxos、Raft 或是 NWR
* 开发和运维的自动化

## 分布式架构之设计模式
### 分布式事务
为了保证不同数据库节点的数据一致性
#### CAP 理论
1. 一致性(Consistency): 指数据在多个副本之间能否保持一致
2. 可用性(Availability): 指系统提供的服务必须一直处于可用状态
3. 分区容错性(P:Partition tolerance): 分布式系统在遇到任何网络分区故障时,仍然需要能够保证对外提供满足一致性和可用性的服务

#### BASE 理论
对CAP中AP的个扩展, 对于业务系统考虑牺牲一致性来换取可用性和分区容错性
1. 基本可用(Basically Available): 通过支持局部故障而不是系统全局故障来实现.如将用户分区在5个数据库服务器上, 一个用户数据库故障只影响这台服务器上的用户,其他用户不受影响
2. 软状态(Soft State): 状态可以有一段时间不同步
3. 最终一致(Eventually Consistent): 最终数据是一致的
强一致。
#### 解决方案
##### 强一致性方案
1. 二阶段提交(2PC)
包括prepare和commit/rollback阶段, 在prepare准备阶段需要等待所有参与子事务的反馈.因此可能造成数据库资源锁定时间过长，不适合并发高以及子事务生命周期较长的业务场景
2. 三阶段提交(3PC)
包括CanCommit,PreCommit和doCommit阶段, 利用超时机制阶段了2PC的同步阻塞问题

##### 弱一致性方案
1. TCC补偿机制
采用了补偿机制, 针对每个操作都注册一个与其对应的确认和补偿操作, 分为Try-Confirm-Cancel
* try阶段: 尝试去执行, 完成所有业务的一致性检查, 预留必须的业务资源
* Confirm阶段: 对业务进行确认提交, 不做任何检查,因为try阶段已经检查过,默认Confirm阶段不会出错
* Cance阶段: 若业务执行失败, 则进入该阶段,它会释放try阶段占用的所有业务资源, 并回滚Confirm阶段执行的所有操作
2. 分布式消息队列: 将分布式事务拆分成本地事务进行处理
* 消息发送方:
    * 首先需要有一个消息表, 记录着消息状态相关信息
    * 业务数据和消息表在同一个数据库, 要保证它俩在同一个本地事务
    * 在本地事务中处理完业务数据和写消息表操作后, 通过写消息到MQ消息队列
    * 消息会发到消息消费方, 如果发送失败, 进行重试
* 消息消费方:
    * 处理消息队列中的消息, 完成自己的业务逻辑
    * 此时如果本地事务处理成功, 则表明已经处理成功了
    * 如果本地事务处理失败, 那么就会重试执行
    * 如果是业务上的失败, 给消息生产方发送一个业务补偿消息, 通知进行回滚等操作

3. 最大努力通知
4. Seata事务框架
* AT模式: 基于本地事务+二阶段协议来实现的最终数据一致性方案(Seata默认)
* TCC模式:
* Saga模式: 长事务解决方案,业务流程中每个参与者都提交本地事务,当出现某一个参与者失败则补偿前面已经成功的参与者
* XA模式: 强一致性的事务解决方法,利用事务资源(数据库、消息服务等)对XA协议的支持,以XA协议的机制来管理分支事务

### 容错设计
容错主要是为了可用性，系统可用性计算： MTTF/MTTF +MTTR（几个9） MTTF = 平均故障前的时间，即系统平均能够正常运行多长时间才发生一次故障 MTTR = 平均修复时间
* 故障隔离->系统分离：按服务种类分离/按用户分离（多租户）

#### 异步通讯
* 请求响应式
    * 发送方会直接请求接收方，被请求方接收到请求后，直接返回——收到请求，正在处理
    * 对于返回结果，一种是发送方时不时地轮询；另一种是发送方注册一个回调方法，接收方处理完后回调请求方
* 通过订阅的方式
    * 接收方订阅发送方的消息，发送方把相关的消息或数据放到接收方所订阅的队列中，接收方会从队列中获取数据
* 通过 Broker 的方式（最佳方式）
    * 发送方向 Broker 发送消息，接收方向 Broker 订阅消息，Broker就是个消息中间件

#### 幂等性
一个调用被发送多次所产生的副作用和被发送一次所产生的副作用是一样的
* 服务调用有三种结果：成功、失败和超时，其中超时是我们需要解决的问题
    * 下游系统提供相应的查询接口,上游系统在 timeout 后去查询一下
    * 查询操作交给下游, 上游只管重试，下游保证一次和多次的请求结果是一样的(幂等)
* 幂等需要有一个全局唯一的标识（Snowflake算法）

##### 幂等性方案
1. select + insert+ 主键/唯一索引冲突
2. insert + 主键/唯一索引冲突
3. 状态机幂等
4. 抽取防重表
5. token令牌
6. 悲观锁(select for update)
7. 乐观锁
8. 分布式锁

#### 补偿事务
#### 重试设计
* 判断什么情况下需要重试，例如，调用超时、被调用端返回了某种可以重试的错误; 业务级的错误（如没有权限、或是非法数据等错误），技术上的错误等就不必重试
* 需要有个重试的最大值，经过一段时间不断的重试后，就没必要再重试了，应该报故障
* 在重试过程中，每一次重试失败时都应该休息一会儿再重试，这样可以避免因为重试过快而导致网络上的负担加重

##### Spring Retry(通过配置 @Retryable 注解)
* 重试策略
    * NeverRetryPolicy，只允许调用1次RetryCallBack，不允许重试
    * AlwaysRetryPolicy，无限重试，直到成功
    * SimpleRetryPolicy，固定次数，默认3次
    * TimeoutRetryPolicy，超时重试，默认1s
    * CircuitBreakerRetryPolicy，熔断重试
* backoff策略
    * NoBackOffPolicy 无退避，立即重试
    * FixedBackOffPolicy 固定时间的退避策略
    * UniformRandomBackOffPolicy 随机时间
    * ExponentialBackOffPolicy 指数退避策略

#### 熔断设计
防止应用程序不断地尝试执行可能会失败的操作
1. 状态
* 闭合:失败计数器在调用失败是+1,当达到阈值就会断开;此时开启超时时钟,在超过该时间后切换到半断（给体系修正错误的机会）
* 半断开:允许一定流量调用服务，调用成功则切换到闭合，调用失败则断开； 有效防止正在恢复中的系统被突然的大量请求拖垮
* 断开:请求会直接返回错误，而不调用后端服务
2. 设计重点
* 错误的类型:什么时候需要重试，什么时候需要直接熔断
* 日志监控:监控使用熔断器保护服务的执行情况
* 测试服务是否可用:定期检查远程服务健康状态
* 手动重置功能:可以手动地强制将熔断器切换到闭合或断开
* 并发问题:最好使用无锁的数据结构或atomic的原子操作
* 资源分区:只对有问题的分区熔断
3. Hystrix

#### 限流设计
对并发访问进行限速，一旦达到阈值，触发限流行为
##### 限流行为
* 拒绝服务:多出来的请求直接拒绝，可以防止一些不正常或恶意高并发访问
* 降级服务:关闭不重要服务，将cpu、内存等让渡给重要的功能； 或者不再返回全量数据，只返回部分数据
* 特权请求:资源让渡给大客户，保证大客户请求能够优先处理
* 延时处理:使用缓冲队列，队列满了拒绝服务，一般用于断在的峰刺请求
* 弹性伸缩:使用自动化运维的方式对相应服务进行伸缩

##### 限流算法
1. 计数器:请求来时+1，请求处理完时-1，当计数器达到阈值就拒绝请求. 
2. 滑动窗口: 本质上也是一种计数器, 通过以时间为维度的滑动窗口设计, 减少临界值带来的并发超过阈值的问题
2. 队列算法:FIFO队列，优先级队列，带权重队列
3. 漏斗算法:在队列中加一个限流器，以匀速处理请求
4. 令牌桶算法:在桶内以一定速率放入token, 处理请求时需要拿到token, 才能处理; 否则排队等待. 流量小时攒token, 流量大时可以快速处理. 如Google的Guava、Redisson
5. 基于响应时间


##### 设计重点
* 为了向用户承诺 SLA\为了应对突发的流\节约成本
* 在架构的早期考虑、限流模块性能必须好、有个手动的开关、有个监控事件通知、返回特定的限流错误码、限流应该让后端的服务感知到

#### 降级设计
* 降低一致性:从强一致性变为最终一致性
    * 使用异步简化流程， 例如：订单系统，当订单量特别大，把在线支付降级为用户到付，从而快速结算订单
    * 降低数据一致性， 使用缓存或者直接去掉数据（不显示库存数量，只显示有无）
* 停止次要功能:限制流量->简化功能->停止功能
* 简化功能:返回全量数据切换为返回部分或最小可用数据
* 设计重点
    * 定义好降级的关键条件，做好相应的应急预案
    * 梳理业务功能，那些是必须死保的功能，那些事可以牺牲掉的功能

#### 负载均衡设计
负载均衡核心目的是让客户端的请求合理均匀的分发到多台目标服务器
##### 负载均衡方案
###### 基于DNS
1. 在DNS服务器上针对某个域名做多个IP映射即可
2. 配置简单, 实现成本低, 无需额外的开发和维护
3. 由于DNS多级缓存的特性, 当我们修改DNS配置之后, 会因为缓存导致IP变更不及时,从而影响负载均衡的效果

###### 基于硬件
1. 性能好, 每秒能够处理百万级别的请求
2. 支持多种负载均衡算法, 我们可以非常灵活的配置不同的负载策略
3. 具备防火墙等安全功能
4. 硬件负载是商业产品, 有专门的售后来支持, 所以企业不需要花精力去做维护

###### 基于软件
Nginx、LVS、HAProxy等
1. 免费
2. 开源, 可做二次开发
3. 灵活性较高

##### 负载均衡作用范围
负载均衡是作用在网络通信上,来实现请求的分发. 而在网络架构中, 基于OSI 模型又分为7层网络模型, 可以在网络的某些分层上做请求分发处理
1. 二层负载: 基于MAC地址来实现请求分发,一般采用虚拟MAC的方式实现.服务器收到请求后通过动态分配后端服务的实际MAC地址进行响应
2. 三层负载：基于IP层负载,一般通过虚拟IP的方式实现. 外部请求访问虚拟IP,服务器收到请求后根据后端实际IP地址进行转发
3. 四层负载: 通过请求报文中的目标地址和端口进行负载, Nginx、F5、LVS等都可以实现四层负载
4. 七层负载: 七层负载是基于应用层负载, 也就是服务器端可以根据http协议中请求的报文信息来决定把请求分
发到哪个目标服务器上, 比如Cookie、消息体、RequestHeader等

##### 负载均衡策略
1. 轮询: 多个服务器按照顺序轮询返回,这样每个服务器都能获得相同的请求次数
2. 随机: 根据随机算法获得一个目标服务地址
3. 一致性Hash: 对于具有相同Hash码的请求, 发送到同一个节点上
4. 最小连接数: 根据目标服务器的请求数量来决定请求分发的权重. 也就是目标服务集群中,请求更少的节点
将会获得更多的请求, 这是负载均衡中比较好的策略, 真正能够实现目标服务器的请求均衡


### 管理设计
#### 配置中心
##### 静态配置:软件启动时配置，运行时基本不变
##### 动态配置:运行时配置，例如： 日志级别、降级开关、活动开关等
* 分类
    * 按运行环境：开发环境、测试环境、预发环境、生产环境
    * 按依赖区分：是否有外部依赖配置（外部依赖配置放在服务发现系统中），例如： mysql链接配置
    * 按层次分：基础层（操作系统配置）、中间平台层（中间件配置）、软件层 （应用自身配置）

#### 部署升级策略
* 停机部署：例如在新版本中使用到了和老版本完全不兼容的数据表设计时就需要停机
* 蓝绿部署：在生产线上部署相同数量的新服务，然后当新的服务测试确认 OK 后，把流量切到新的服务这边来
* 滚动部署：逐个替换应用的所有实例
* 灰度部署：逐渐将生产环境流量从老版本切换到新版本 通常在对新版本质量缺乏信心或没有足够测试或缺少可靠测试时使用
* AB测试：同时上线两个版本，然后做相关的比较 用于测试应用功能表现（可用性、受欢迎程度等，例如：UI大改、推荐算法、流程等更改）

#### 网关模式
* 请求路由
* 服务注册， 注册 API 接口
* 安全方面， SSL 加密及证书管理、Session 验证、授权、数据校验
* 负载均衡
* 异步、重试、幂等、流控、熔断、监视
* API聚合、API编排

#### 分布式锁
是一种跨进程的跨机器节点的互斥锁,它可以用来保证多机器节点对于共享资源访问的排他性
* 特点:安全性、避免死锁、容错性
* Redis
* zookeeper

#### 边车模式

### 性能设计
#### 缓存
##### 模式
* Cache Aside:先从cache取数据，取到则返回，没有则从DB取数据，成功后xi，将数据放入cache； 更新则是先存入DB，成功后，让缓存失效
* Read/Write Through
    * ReadThrough是在查询操作中更新缓存，当缓存失效时，由缓存服务自己加载，cache aside则是由调用方将数据载入缓存
    * WriteThrough是在更新数据时，没命中缓存则直接更新DB，命中缓存，则更新缓存，然后由缓存更新DB
* Write Behind Caching:更新数据的时候，只更新缓存，不更新数据库，缓存会异步地批量更新数据库

##### 设计重点
* 不能在 Service 内放 Local Cache、外部的缓存集群、数据分片、命中率、缓存时间周期、LRU 策略

#### 异步处理
事件驱动、事件溯源
#### 数据库扩展
* 读写分离CQRS:Command(增、删、改)/Query(查)
* 分库分表
    * 分库策略: 地区、日期、范围、哈希散列算法
    * 分表策略: 用户ID、数据种类、范围

#### 秒杀
1. 存在的问题
* 瞬时请求量极大
* 恶意请求
* 链接暴露
* 数据库抗压
* 超卖
2. 解决办法
* 页面静态化: 秒杀活动页面大多内容时固定的, 对页面做静态化处理, 减少访问服务端的请求
* 按钮置灰控制: 秒杀前按钮置灰以防止秒杀前的大量无用请求
* 秒杀链接加密: 防止提前秒杀和恶意请求
* MQ异步处理: 消息队列削峰
* 网关限流、过滤ip等
* 熔断降级
* 使用redis分布式锁解决超卖问题
* CDN边缘节点计算，在CDN上部署个小服务，除了告诉前端请求开没开始外，还统计多少人在线，每隔段时间将数据回传给数据中心，当快开始的时候，数据中心向CDN上的小服务传递一个概率值，例如：100万人，0.02%概率，则200人抢，其他返回秒杀结束

#### 边缘计算
处理实时响应业务、简单业务、收集并结构化数据等

## 云服务
利用互联网提供动态易扩展的虚拟化资源整合服务.云服务的主体架构主要有基础设施服务、平台服务、软件服务
1. IaaS: 基础设施即服务
2. PaaS: 平台即服务
3. SaaS: 软件即服务

### IaaS、PaaS、SaaS区别
1. IaaS是面向企业或开发者, 提供基础资源支持, 包括：计算、存储、网络等
2. PaaS面向开发者, 提供软件运行的平台环境, 或者以API、SDK的形式开放给客户调用. 包括：数据分析、人工能、Docker; 推送、通信、语音识别、图像识别、统计、广告等. PaaS是在IaaS的基础之上解决了操作系统、数据库、运行时环境、中间件、各种框架的搭建操作问题
3. SaaS面向企业或个人终端用户, 通过网络租用的形式提供软件, 包括: 管理类应用、业务类应用、行业类应用等等.SaaS可以调用PaaS层的能力, 也可以使用IaaS层的资源独立开发
4. 从Iaas到SaaS的过程, 企业需要付出的开发成本越来越小, 云服务化的程度也越来越高




## 微服务
### 微服务演进
#### 第一代
1. 当我们把一个电商系统以微服务化架构进行拆分后,会的到这样的一个架构,其中包括WebServer、Payment、inventory等等. 这些微服务应用会被部署到Docker容器或Kubernetes集群. 
2. 由于每个服务的业务逻辑是独立的, 所以服务之间必须要相互通信, 才能实现功能的完整性. 比如用户把一个商品加入购物车,请求会进入到Webserver, 然后转发到shopping cart进行处理, 最后存到数据库.
3. 在这个过程中, 每个服务之间必须要知道对方的通信地址, 并且当有新的节点加入进来的时候, 还需要对这些通信地址进行动态维护. 所以每个微服务除了要实现业务逻辑以外, 还需要解决上下游寻址、通讯、以及容错等问题

#### 第二代
1. 因为第一代的问题, 在第二代微服务中引入了服务注册中心来实现服务之间的寻址, 并且服务之间的容错机制、负载均衡也逐步形成了独立的服务框架, 比如Spring Cloud
2. 在第二代微服务架构中, 业务开发不仅仅需要关注业务逻辑, 还需要处理微服务中的一些基础性配置工作,虽然Spring Cloud已经尽可能去完成了这些事情,但对于开发人员来说,学习Spring Cloud以及针对Spring Cloud的配置和维护仍然存在较大的挑战. 同时增加了整个微服务的复杂性

#### 第三代(Service Mesh服务网格)——云原生时代
1. 微服务中的服务注册、容错、重试、安全等工作都是为了保证服务之间通信的可靠性, 因此有了service Mesh
2. 原本模块化到微服务框架里的微服务基础能力, 被进一步的从一个SDK中演进成了一个独立的代理进程(SideCar)
3. SideCar的主要职责就是负责各个微服务之间的通信, 承载了原本第二代微服务架构中的服务发现、容错、服务治理等功能, 使得微服务基础能力和业务逻辑迭代彻底解耦

### 微服务之电商平台系统设计
1. 如何划分微服务
* 业务领域: 用户、社区、商品等模块
* 单一功能职责: 订单、支付、物流、权限
* 重要程度: 核心非核心

## 云原生
即Cloud Native, Cloud表示应用程序位于云中; Native表示应用程序从设计之初即考虑到云的环境
### 四要素
1. 微服务
2. 容器化: docker (K8s是容器编排系统, 用于容器管理, 容器间的负载均衡)
3. 支持DevOps: 开发、测试、运维的合体. DevOps为云原生提供持续交付能力
4. 满足持续交付(CICD): 相当于要实现在线不停机更新, 要求开发版本和稳定版本并存, 需要标准的流
程和工具支撑. 目前能够提供持续交付的工具也很多, 比如Jenkins、Sonar等

### Pulsar
云原生分布式消息流平台, 它集消息、存储、轻量化函数式计算为一体
#### 特性
1. 云原生架构
* Pulsar采用计算与存储分离的云原生架构. 数据从Broker搬离, 存在共享存储内部. 上层是无状态Broker复制消息分发和服务; 下层是持久化的存储Bookie集群. Pulsar存储是分片的, 这种构架可以避免扩容时受限制,实现数据的独立扩展和快速恢复
2. 跨区域大集群
* 大集群就是跨机房、跨地域的集群. 使得Pulsar的分布式能力不局限于某个机房
3. 多租户模式
* Pulsar是一个多租户系统. 租户可以跨集群分布, 每个租户都可以有单独的认证和授权机制. 租户也是存储配额、消息TTL和隔离策略的管理单元
4. 统一的存储模型
* Pulsar提供了统一的消息存储模型, 支持对主流的消息中间件的兼容和接入
5. 统一的消费模型
* Pulsar提供统一的消费模型. 一共支持四种消费模式:独占模式(Exclusive)、灾备模式(Failover)、共享订阅(Shared)、键共享订阅(Key_Shared)
6. 分片流
* Pulsar将无界的数据看作是分片的流, 分片分散存储在分层存储(Tiered Storage)、BookKeeper集群和Broker节点上, 而对外提供一个统一的、无界数据的视图
7. 跨地域复制
* Pulsar中的跨地域复制是将Pulsar中持久化的消息在多个集群间备份. 在Pulsar2.x中新增了复制订阅模式, 在某个集群失效情况下, 这个功能可以在其他集群恢复消费者的消费状态, 从而达到热备模式下消息服务的高可用
8. IO连接器
* Pulsar IO支持非常多数据流的连接集成操作. 例如HDFS 、Spark、Flink 、Flume 、ES 、HBase、MySQL、Redis、MongoDB、Kafka、RocketMQ、Rabbit、ActiveMQ、Netty等
9. 轻量级计算框架
* Pulsar可以给用户提供一个部署简单、运维简单、API 简单的FASS(Function as a service)平台. 对复杂的大数据处理框架的有力补充