---
layout: post
title:  "Zookeeper知识盘点"
date:   2023-04-13 09:00:30 +0200
categories: zookeeper
---

Zookeeper重要知识盘点, 包括Leader选举、数据同步、监听机制、数据模型等

## 分布式集群环节中的挑战
1. 集群管理
为保证集群的高可用, 集群中每个节点都会保存一个数据副本, 客户端访问任意节点都能得到最近数据
2. 分布式锁
保证共享资源的并发安全
3. Master选举
为降低集群数据同步复杂度, 一般会设置Master和Slave两种角色, Master负责读写操作, Slave负责读操作. 因此需要从节点中选举出Master

## Zookeeper简介
分布式协调组件, 本质上是一个分布式小文件存储系统, 主要解决分布式环境中应用系统的一致性问题, 可以用做统一配置管理、统一命名服务、分布式锁、集群管理等
### 架构成员
1. Leader——核心角色
* 集群内部各个服务器的调度者
* 事务请求的唯一调度和处理者, 保证集群事务处理的顺序性
2. Follower
* 处理非事务请求
* 将事务请求转发给Leader
* 参与集群Leader选举
3. Observer
* 观察集群最新状态变化并同步更新状态
* 处理非事务请求, 将事务请求转发给Leader
* *不参与集群Leader选举*

### 工作状态
1. LOOKING: 寻找Leader状态, 进入Leader选举
2. FOLLOWING: 表明当前服务器角色为Follower
3. LEADING: 表明当前服务器角色为Leader
4. OBSERVING: 表明当前服务器角色为Observer

### 特性
1. 顺序一致性: 从同一客户端发起的事务请求, 最终将会严格按照顺序应用到zk中
* 采用了全局递增的事务Id来标识, Leader收到请求后, 把请求封装成事务proposal, 并分配一个zxid
* zxid是一个64位的数字,高32位是epoch用来标识leader周期, 如果有新的leader产生出来, epoch会自增; 低 32位用来递增计数
* 当新产生proposal时, 会依据数据库的两阶段过程, 首先向所有follower发出事务执行请求,如果超过半数的机器都能执行并且能够成功, 那么就会开始执行
2. 原子性: 所有事务请求的处理结果在集群中一致
3. 单一视图: 无论客户端连接到哪个节点, 其看到的服务端数据一致
4. 可靠性: 一旦服务器成功应用了一个事务, 并完成对客户端的响应, 该事务引起的服务端状态变更将一直保持
5. 实时性(最终一致性): 客户端最终一定能从服务端读到最新数据状态

## Zookeeper集群部署
一主多从结构, 写请求由主服务器处理, 再通知从服务器; 读请求既可以读主服务器, 也可以读从服务器
### 主从数据同步
采用ZAB(原子广播协议)来保证主从数据一致性, 支持崩溃恢复和消息广播两种模式
1. 崩溃恢复
* 当Leader崩溃, 进入该模式, 所有服务器进入LOOKING状态, 并进行Leader选举
* 选出新Leader后, 集群中其他服务器跟Leader服务器进行数据同步
* 当大部分服务器已经与Leader同步完成后, 退出恢复模式, 进入广播模式
2. 广播模式
* Leader开始处理事务请求, 并把更新数据同步到其他服务器
* Leader将请求封装成⼀个事务Proposal(提议),将其发送给所有Follwer
* 如果收到超过半数反馈ACK, Proposal被允许
* Leader发送commit到follower, 同时自己commit

### Leader选举
1. 每个服务器都将自己作为Leader投票, 也即每个服务器会投票给自己, 投票元素为: epoch, myid, zxid
* epoch: 逻辑时钟, 当前投票是否过期, 过期丢弃, 否则影响投票结果和效率
* myid: 服务器id, 在myid文件中填写的数字, 避免投票时间过长
* zxid: 事务id, 当前节点最新存储的数据事务编号, zxid越大说明该节点数据越接近Leader数据, 避免数据丢失
2. 每个服务器将投票发给集群中其他服务器, 同时校验投票有效性: 是否本轮投票、是否来自LOOKING状态服务器
3. 收到其他服务器的投票后, 将其他投票与自己投票PK
* 优先比较zxid, zxid大的优先作为Leader
* 如果zxid相同, 比较myid, myid大的优先作为leader
4. 统计投票, 判断是否有过半数服务器收到相同投票信息, 有则选出Leader, 没有则进入下一轮投票

***假设集群有5台服务器, myid依次为1,2,3,4,5, 其中没有Observer角色***
#### 首次启动
1. 5台服务器依序启动, 因首次启动没有历史数据, 因此zxid均为0
2. 服务器1启动, 当前1台服务器, 无法达到过半数投票, 因此服务器1选举状态为LOOKING
3. 服务器2启动, 当前2台服务器, 无法达到过半数投票, 因此选举状态都为LOOKING
3. 服务器3启动, 当前3台服务器, 可以达到过半数投票, 以myid大者胜出, 因此服务器3被选举为Leader, 服务器1,2进入FOLLOWING状态
4. 服务器4启动, 因之前已有过半数服务器选举了服务器3,即使服务器4的myid最大, 也只能少数服从多少
5. 服务器5启动, 同上

#### 非首次启动
1. Leader宕机, 服务器1,2,4,5进入LOOKING状态, 开始Leader选举
2. 服务器1,2,4,5分别投票给自己, 假设投票数据为(1, 111), (2, 222), (4, 444), (5, 555)
3. 根据投票规则, 服务器5胜出, 当选Leader
4. 服务器1,2,4进入FOLLOWING状态, 服务器5进入LEADING状态

## Zookeeper数据模型
zk的数据结构桶unix文件系统类似, 也是树状结构, 每个路径都是唯一的, 节点统一叫znode, 通过路径标识
  <img src= "/assets/files/zk数据模型.png" alt="加载错误" title="zk数据模型"/>

### znode类型
1. 持久节点: 创建后一直存在于zk服务器上, 直到手动删除
2. 持久顺序节点: 基于持久节点, 节点后会追加1个在父节点维护了一个自增整数, 以表示子节点创建先后顺序
3. 临时节点: 客户端与服务端断开连接就自动删除, 临时节点只能作为叶子节点
4. 临时顺序节点: 基于临时节点, 加入顺序特性

### znode内容
每个节点数据最大不能超过1M, 最好小于1k. 保证高吞吐和低延迟
1. byte data[]: 存储的业务数据
2. Long acl: 记录客户端对znode的访问权限
3. StatPersisted stat: 节点状态信息, 包括zxid, version, timestamp
4. Set<String> children: 当前节点的子节点引用(默认null)

### Watcher监听机制
ZooKeeper允许客户端向服务端某个znode注册⼀个Watcher监听,当服务端的⼀些指定事件触发了Watcher,那么Zk就会向指定客户端发送⼀个事件通知来实现分布式的通知功能. 
#### 具体流程
1. 客户端在向**Zookeeper服务器**注册的同时, 会将Watcher对象存储在**客户端WatcherManager**当中
2. 当Zookeeper服务器触发Watcher事件后, 会向客户端发送通知
3. **客户端线程**从WatcherManager中取出对应的Watcher对象来执⾏回调逻辑

#### 特性
1. 一次性: 一个Watcher触发后, 就被移除. 可以有效减轻服务端压力,不然对于更新非常频繁的节点,服务端会不断的向客户端发送事件通知,无论对于网络还是服务端的压力都非常大
2. 客户端串行化执行: 回调的过程是串行同步的过程
3. 轻量级: Watcher只通知客户端发生了事件, 而不会传递事件的具体内容

#### 客户端注册
1. 调用getData()/getChildren()/exist(),传入Watcher对象
2. 封装 Watcher到WatchRegistration
3. 封装成Packet对象,向服务端发送request
4. 收到服务端响应后, 将 Watcher注册到WatcherManager中进行管理
5. 请求返回,完成注册

#### 服务端处理
1. 服务端接收Watcher并存储
2. Watcher触发
3. 调用process方法发出通知
#### 客户端回调
1. 客户端SendThread线程接收事件通知, 交由EventThread线程回调Watcher

## 功能
### 命名服务
指通过指定的名字来获取资源或服务的地址, 利用zk创建一个全局的路径, 这个路径就可以作为一个名字指向相应资源或服务地址
### 统一配置管理
利用watcher机制, 配置文件放在znode上, 应用服务监听这个znode就可以及时响应配置变化
### 集群状态管理
利用临时节点特性,监控服务存活状态
### 分布式锁
利用临时顺序节点特性实现分布式锁
#### 获取锁
所有客户端请求都在zk上的同一节点(locks)下创建临时顺序节点
1. client1 尝试获取锁, 在locks节点下创建lock1
2. client1 会查找locks下所有临时顺序节点, 并判断lock1是否排序最小, 如果是, 则获取锁
3. client2 尝试获取锁, 在locks节点下创建lock2
4. client2 查找locks下所有临时顺序节点, 并判断lock2是否排序最小, 发现lock1才是最小的, 因此获取锁失败
5. client2 向当前节点(lock2)的上一个节点(lock1)注册watcher事件, 监听lock1是否存在, 即进入等待状态

#### 释放锁
1. client1 任务完成或故障时, 都会删除临时节点(前者显示调用, 后者自动删除)
2. client2 监听到lock1删除事件后, 查找locks下所有节点并判断lock2是最小的, 于是获得锁

## Zookeeper与Redis分布式锁的有缺点
1. Redis是基于它的命令执行是单线程的这个特性来做分布式锁的
* 获取锁的方式简单粗暴, 如果获取不到锁, 会不断尝试获取锁, 比较消耗性能
* Redis是AP模型, 在集群模式中由于数据的一致性会导致锁出现问题, 即便使用Redlock算法来实现,在某些复杂场景下也无法保证其实现100%的可靠性
* 其中Redisson客户端对分布式锁进行了很好的包装, redisson分布式锁基于lua+hash来实现加锁逻辑,同时利用Redis的发布与订阅以及seamphore来实现锁的等待. 并且添加了锁续期等功能
2. zk的分布式锁, 主要是基于zk的临时有序节点+watch机制来实现的
* Zk中每个线程去抢占锁, 都会去创建一个临时节点, 当抢占锁的线程比较多的时候,对ZK集群的压力会比较大;而Redis则是通过判断key是否存在来获取锁
* zk获取不到锁,只需要添加一个监听器就可以了,不用一直轮询,性能消耗较小

## dubbo与zookeeper

## Nacos
### Nacos选举机制-Raft算法
它通过一切以Leader为准的方式, 实现一系列值的共识和各节点日志的一致
#### 角色及任期
1. Follower: 接收和处理来自Leader的消息, 当等待Leader心跳信息超时的时候, 就主动站出来,推荐自己当 Candidate
2. Candidate: 向其他节点发送投票请求, 通知其他节点来投票, 如果赢得了过半数选票, 就晋升Leader
3. Leader: 负责处理客户端请求, 进行日志复制等操作, 每一轮选举的目标就是选出一个Leader, Leader会不断地发送心跳信息, 通知其他节点不需要发起新的选举
4. Term: 任期

#### 选举过程
1. 在初始阶段, 集群中所有节点都是Follower 状态, 都被设定一个随机选举超时时间(一般 150ms-300ms)
  <img src= "/assets/files/Raft选举算法初始化.jpg" alt="加载错误" title="Raft选举算法初始化"/>

2. 如果Follower在规定的超时时间都没有收到来自Leader心跳,它就发起选举,将自己的状态切为Candidate,增加
自己的任期编号,然后向集群中其它节点发送请求, 询问其是否选举自己成为Leader
  <img src= "/assets/files/Raft选举算法发起投票.jpg" alt="加载错误" title="Raft选举算法发起投票"/>

3. 其他节点收到候选节点请求投票消息后, 如果在编号为1的这届任期内还没有进行过投票,那么它将把选票投给候选节点, 并增加自己的任期编号
  <img src= "/assets/files/Raft选举算法投票.jpg" alt="加载错误" title="Raft选举算法投票"/>

4. 当收到来自集群中过半节点的投票后, 候选节点即成为本届任期内Leader, 将周期性地发送心跳信息, 通知其他节点其是Leader, 阻止Follower发起新的选举
  <img src= "/assets/files/Raft选举算法Leader.jpg" alt="加载错误" title="Raft选举算法Leader"/>

#### 日志复制
  <img src= "/assets/files/Raft日志项.jpg" alt="加载错误" title="Raft日志项"/>
  <img src= "/assets/files/Raft日志复制.jpg" alt="加载错误" title="Raft日志复制"/>
1. 当系统(leader)收到一个来自客户端✁写请求, 会添加一个log entry(日志项)到本地日志
2. Leader通过日志复制(AppendEntries)RPC 消息, 将日志项并行复制到集群其它Follower节点
3. 如果Leader接收到大多数复制成功的响应后, 它将日志项应用到自己的状态机, 并返回成功给客户端;如果Leader没有接收到大多数复制成功的响应, 那么就返回错误给客户端
4. 当Follower接收到心跳信息, 或者新的AppendEntries消息后, 如果发现Leader已经提交了某条日志项, 而自己还没应用, 那么Follower就会将这条日志项应用到本地状态机中

### Nacos配置动态更新原理
Nacos采用的是长轮询的方式. 由Nacos Client向Nacos Server端去发起配置更新查询的请求
* 长轮询: 就是客户端发起一次轮询请求到服务器端, 当服务器端的配置没有任何变更的时候, 这个连接会一直打开, 直到服务端有配置变更或者连接超时之后才返回
#### 具体过程
1 将客户端本地缓存的配置信息和服务器端获取的配置信息进行比较, 一旦发现本地缓存的配置内容和服务端的配置内容有差异, 就表示服务器端的配置有更
2 把更新的配置拉到本地
#### 对比时间优化
在上述过程中, 有可能因为客户端的配置比较多, 而导致对比的时间较长, 使得配置的同步效率非常低. Nacos做了两方面优化
1. 减少网络通信的数据量
* 客户端把需要进行对比的配置按配置项进行分片, 每个分片的大小是3000项, 也就是说每一次最多拿3000个配置项去Nacos Server端进行对比
2. 分阶段进行对比和更新
* 第一阶段, 客户端把3000个配置项的Key以及对应Value的MD5值拼接成一个字符串,然后发送到Nacos Server端进行判断,服务端会逐个比较这些配置中MD5不同的Key, 把存在更新的Key返回给客户端
* 第二阶段, 客户端拿到这些数据有变更的Key, 循环逐个调用服务端, 从而获取这些Key对应的Value值